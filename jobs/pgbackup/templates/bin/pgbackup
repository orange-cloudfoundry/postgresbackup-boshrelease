#!/bin/bash

chmod 777 ../shell/env
../shell/env
PATH=/var/vcap/bosh/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
set +e # do not want to auto-die - instead, log and continue
# IMPROVEMENTS: use pgbackup.debug --> Not implemented yet
TIMESTAMP=$(date +%Y/%m/%d/%Y%m%d%H%M%S)

# Environment for postgres commands
PGHOST="<%= p('pgbackup.host') %>" # host  for the postgres database url
PGPASSWORD="<%= p('pgbackup.password') %>" #  port associated with the postgres database url

<%
pgversion = case p('pgbackup.version')
when 9.5, "9.5"
  "postgres95"
when 9.4, "9.4"
  "postgres94"
when 9.3, "9.3"
  "postgres93"
when 9.2, "9.2"
  "postgres92"
when 9.1, "9.1"
  "postgres91"
when 9.0, "9.0"
  "postgres90"
else
  "postgres95"
end
%>

# ==> pgversion default value will be removed
PGVERSION="<%= pgversion %>"
export PGHOST PGPASSWORD PGVERSION
echo "export PGHOST=$PGHOST"
echo "export PGVERSION=$PGVERSION"

PG_DUMP="/var/vcap/packages/${PGVERSION}/bin/pg_dump"

GOF3R="/var/vcap/packages/gof3r/bin/gof3r"

# S3 bucket access credentials
AWS_ACCESS_KEY_ID="<%= p('pgbackup.s3.access_key_id') %>"
AWS_SECRET_ACCESS_KEY="<%= p('pgbackup.s3.secret_access_key') %>"
export AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY

# S3 bucket other information
S3_BUCKET="<%= p('pgbackup.s3.bucket') %>"
S3_PATH="/<%= p('pgbackup.s3.path') %>"
S3_ENDPOINT="<%= p('pgbackup.s3.endpoint') %>"
#get date and time to use it for backup file name
T=$(date +"%Y-%m-%d__%H-%M")

log () {
  level=$1
  msg=$2
  logger -t "$LOGGER_TAG" -p user.$level "[$T] $msg"
}

backup_databases() {
        # Acquire a lock to ensure we aren't running simultaneous dumps
        echo "backup in progress "  > /tmp/backup.lock
        local opts="--endpoint=${S3_ENDPOINT}"
        local name="<%= p('pgbackup.name') %>"
        s3_file=$(echo "${name}-$T".gz | sed 's|/\+|/|g' )
        dump_file="$name.dump"
        ${PG_DUMP} --username="<%= p('pgbackup.username') %>" --port="<%= p('pgbackup.port') %>" -f $dump_file
        # Verify the creation of the backup file
        if [[ $? == 0 ]]; then
                log "ERROR" "$T : Failed to dump database, you may verify the database user/password credentials"
        fi
        gzip $dump_file
        mv "$dump_file.gz" $s3_file
        ${GOF3R}  cp  ${opts} "$s3_file" s3://"$S3_BUCKET/$s3_file" >/dev/null 2>&1
        # ETag-hash comparison always fail, so related error is skipped
        # see more : Etag-hash s3 multipart
        echo "backed up successfully"
        rm $s3_file

}


echo "pgbackup starting up"
# Test if there is no current backup
# if a backup.lock file exist then we assume that an other dump is running, so we will retry to backup in the next run

if [ ! -f "/temp/backup.lock" ]; then
    backup_databases
else
    log "WARNING : an other dump is running or the last backup failed , see logs for more details "
fi

# release lock for the next run
rm -f backup.lock

echo "pgbackup shutting down"
exit 0
