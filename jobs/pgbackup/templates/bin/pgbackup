#!/bin/bash

chmod 777 ../shell/env
../shell/env
PATH=/var/vcap/bosh/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
# proxy setting is loaded from the ../shell/env
set +e # do not want to auto-die - instead, log and continue
# IMPROVEMENTS: use pgbackup.debug --> Not implemented yet

<%
pgversion = case p('pgbackup.version')
when 9.5, "9.5"
  "postgres95"
when 9.4, "9.4"
  "postgres94"
when 9.3, "9.3"
  "postgres93"
when 9.2, "9.2"
  "postgres92"
when 9.1, "9.1"
  "postgres91"
when 9.0, "9.0"
  "postgres90"
else
  "postgres95"
end
%>
PGVERSION="<%= pgversion %>"

# Environment for postgres commands
PGHOST="<%= p('pgbackup.host') %>" # host  for the postgres database url
PGPASSWORD="<%= p('pgbackup.password') %>" #  port associated with the postgres database url
EXTRA_ARG="<%= p(' pgbackup.pgdump.arguments') %>"
# S3 bucket access credentials
AWS_ACCESS_KEY_ID="<%= p('pgbackup.s3.access_key_id') %>"
AWS_SECRET_ACCESS_KEY="<%= p('pgbackup.s3.secret_access_key') %>"

export PGHOST PGPASSWORD PGVERSION AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY

# S3 bucket other informations
S3_BUCKET="<%= p('pgbackup.s3.bucket') %>"
S3_PATH="/<%= p('pgbackup.s3.path') %>"
S3_ENDPOINT="<%= p('pgbackup.s3.endpoint') %>"
#get date and time to use it for backup file name
T=$(date +"%Y-%m-%d__%H-%M")
PG_DUMP="/var/vcap/packages/${PGVERSION}/bin/pg_dump"
GOF3R="/var/vcap/packages/gof3r/bin/gof3r"
DUMP_DIR="/var/vcap/data/pgbackup/"

#if ssl encryption is setup to false then ssl_status_opt = --no-ssl
ssl_status_opt=""
ssl_status_flag= "<%= pgbackup.proxy.https.ssl %>"
if [[ "$ssl_status_flag" == "true" ]]; then
ssl_status_opt="--no-ssl"
fi
# ==> all logs are sent to '/var/vcap/log/jobs/pgbackup' (see ../shell/env file)
function backup_databases() {
        # Acquire a lock to ensure we aren't running simultaneous dumps
        echo "backup in progress"  > /tmp/backup.lock
        opts="--endpoint=${S3_ENDPOINT} ${ssl_status_opt}"
        name="<%= p('pgbackup.name') %>"
        s3_file=$(echo "${name}-$T".gz | sed 's|/\+|/|g' )
        dump_file="$name.dump"
        ${PG_DUMP} ${EXTRA_ARG} --username="<%= p('pgbackup.username') %>" --port="<%= p('pgbackup.port') %>" -f "${DUMP_DIR}${dump_file}"
        # Verify the creation of the backup file
        if [[ $? == 0 ]]; then
                echo "ERROR" "$T : Failed to dump database, you may verify the database user/password credentials"
        fi
        gzip "${DUMP_DIR}$dump_file
        mv "${DUMP_DIR}"$dump_file.gz" "${DUMP_DIR}$s3_file
        ${GOF3R}  cp  ${opts} ""${DUMP_DIR}$s3_file" s3://"$S3_BUCKET/$s3_file" >/dev/null 2>&1
        # ETag-hash comparison always fail, so related error is skipped
        # see more : Etag-hash s3 multipart
        echo "backed up successfully"
        rm "${DUMP_DIR}$s3_file
}


echo "pgbackup starting up"
# Test if there is no current backup
# if a backup.lock file exist then we assume that an other dump is running, so we will retry to backup in the next run

if [ ! -f "/tmp/backup.lock" ]; then
    backup_databases
else
    echo "ERROR" " : an other dump is running or the last backup failed , see logs for more details"
fi

# release lock for the next run

rm -f "/tmp/backup.lock"

echo "pgbackup shutting down"
exit 0
