#!/bin/bash

chmod 777 ../shell/env
. ../shell/env
PATH=/var/vcap/bosh/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
# proxy setting is loaded from the ../shell/env
set +e # do not want to auto-die - instead, log and continue
# IMPROVEMENTS: use pgbackup.debug --> Not implemented yet

<%
pgversion = case p('pgbackup.version')
when 9.5, "9.5"
  "postgres95"
when 9.4, "9.4"
  "postgres94"
when 9.3, "9.3"
  "postgres93"
when 9.2, "9.2"
  "postgres92"
when 9.1, "9.1"
  "postgres91"
when 9.0, "9.0"
  "postgres90"
else
  "postgres95"
end
%>

PGVERSION="<%= pgversion %>"

# Environment for postgres commands
PGHOST="<%= p('pgbackup.host') %>" # host  for the postgres database url
PGPASSWORD="<%= p('pgbackup.password') %>" #  port associated with the postgres database url
EXTRA_ARG="<%= p('pgbackup.pgdump.arguments') %>"
# S3 bucket access credentials
AWS_ACCESS_KEY_ID="<%= p('pgbackup.s3.access_key_id') %>"
AWS_SECRET_ACCESS_KEY="<%= p('pgbackup.s3.secret_access_key') %>"

export PGHOST PGPASSWORD PGVERSION AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY

# S3 bucket other informations
S3_BUCKET="<%= p('pgbackup.s3.bucket') %>"
S3_PATH="/<%= p('pgbackup.s3.path') %>"
S3_ENDPOINT="<%= p('pgbackup.s3.endpoint') %>"
#get date and time to use it for backup file name
T=$(date +"%Y-%m-%d__%H-%M")
PG_DUMP="/var/vcap/packages/${PGVERSION}/bin/pg_dump"
GOF3R="/var/vcap/packages/gof3r/bin/gof3r"
DUMP_DIR="/var/vcap/data/jobs/pgbackup/"




function backup_databases() {
        # Acquire a lock to ensure we aren't running simultaneous dumps
        echo "backup in progress"  > /tmp/backup.lock
        local opts="--endpoint=${S3_ENDPOINT}"
        local name="director_db"
        s3_file=$(echo "${name}-$T".tgz | sed 's|/\+|/|g' )
        dump_file="$name.sql"
        ${PG_DUMP} ${EXTRA_ARG} --username="<%= p('pgbackup.username') %>" --port="<%= p('pgbackup.port') %>" -f "${DUMP_DIR}${dump_file}"
        # Verify the creation of the backup file
        if [[ $? != 0 ]]; then
                echo "ERROR Failed to dump database, you may verify the database user/password credentials"
        fi
        gzip "${DUMP_DIR}${dump_file}"
        cd ${DUMP_DIR}
        tar -zcf "${dump_file}.tgz" "${dump_file}"
        cd - # return to the previous directory
        mv "${DUMP_DIR}${dump_file}.gz" "${DUMP_DIR}${s3_file}"
        ${GOF3R}  cp  ${opts} "${DUMP_DIR}$s3_file" s3://"$S3_BUCKET/$s3_file" >/dev/null 2>&1
        # ETag-hash comparison always fail, so related error is skipped
        # see more : Etag-hash s3 multipart
        echo "backed up successfully"
        rm ${DUMP_DIR}$s3_file
        # release the lock for the next run
        rm -f "/tmp/backup.lock"

}


echo "pgbackup starting up"
# Test if there is no current backup
# if a backup.lock file exist then we assume that an other dump is running, so we will retry to backup in the next run

if [ ! -f "/tmp/backup.lock" ]; then
    backup_databases
else
    echo "ERROR : an other dump is running or the last backup failed , see logs for more details"
fi


echo "pgbackup shutting down"
exit 0